{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch>=1.9.0 (from -r requirements.txt (line 2))\n",
      "  Using cached torch-2.8.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision>=0.10.0 (from -r requirements.txt (line 3))\n",
      "  Using cached torchvision-0.23.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./env36/lib/python3.13/site-packages (from -r requirements.txt (line 4)) (2.3.2)\n",
      "Collecting pandas>=1.3.0 (from -r requirements.txt (line 5))\n",
      "  Using cached pandas-2.3.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: Pillow>=8.3.0 in ./env36/lib/python3.13/site-packages (from -r requirements.txt (line 6)) (11.3.0)\n",
      "Collecting opencv-python>=4.5.0 (from -r requirements.txt (line 7))\n",
      "  Using cached opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting scikit-learn>=1.0.0 (from -r requirements.txt (line 8))\n",
      "  Using cached scikit_learn-1.7.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting matplotlib>=3.4.0 (from -r requirements.txt (line 9))\n",
      "  Using cached matplotlib-3.10.5-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn>=0.11.0 (from -r requirements.txt (line 10))\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyyaml>=5.4.0 (from -r requirements.txt (line 13))\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting omegaconf>=2.1.0 (from -r requirements.txt (line 14))\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tensorboard>=2.7.0 in ./env36/lib/python3.13/site-packages (from -r requirements.txt (line 17)) (2.20.0)\n",
      "Collecting wandb>=0.12.0 (from -r requirements.txt (line 18))\n",
      "  Using cached wandb-0.21.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting tqdm>=4.62.0 (from -r requirements.txt (line 19))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting timm>=0.4.0 (from -r requirements.txt (line 22))\n",
      "  Using cached timm-1.0.19-py3-none-any.whl.metadata (60 kB)\n",
      "Collecting torchmetrics>=0.5.0 (from -r requirements.txt (line 23))\n",
      "  Using cached torchmetrics-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting pytest>=6.2.0 (from -r requirements.txt (line 26))\n",
      "  Using cached pytest-8.4.1-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting black>=21.9.0 (from -r requirements.txt (line 27))\n",
      "  Using cached black-25.1.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n",
      "Collecting flake8>=3.9.0 (from -r requirements.txt (line 28))\n",
      "  Using cached flake8-7.3.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting isort>=5.9.0 (from -r requirements.txt (line 29))\n",
      "  Using cached isort-6.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jupyter>=1.0.0 (from -r requirements.txt (line 32))\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting notebook>=6.4.0 (from -r requirements.txt (line 33))\n",
      "  Using cached notebook-7.4.5-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting ipywidgets>=7.6.0 (from -r requirements.txt (line 34))\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting onnx>=1.10.0 (from -r requirements.txt (line 37))\n",
      "  Using cached onnx-1.18.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting onnxruntime>=1.9.0 (from -r requirements.txt (line 38))\n",
      "  Using cached onnxruntime-1.22.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting fastapi>=0.70.0 (from -r requirements.txt (line 41))\n",
      "  Using cached fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting uvicorn>=0.15.0 (from -r requirements.txt (line 42))\n",
      "  Using cached uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: tensorflow>=2.13.0 in ./env36/lib/python3.13/site-packages (from -r requirements.txt (line 43)) (2.20.0rc0)\n",
      "Collecting filelock (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./env36/lib/python3.13/site-packages (from torch>=1.9.0->-r requirements.txt (line 2)) (4.14.1)\n",
      "Requirement already satisfied: setuptools in ./env36/lib/python3.13/site-packages (from torch>=1.9.0->-r requirements.txt (line 2)) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached triton-3.4.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env36/lib/python3.13/site-packages (from pandas>=1.3.0->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.3.0->-r requirements.txt (line 5))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.3.0->-r requirements.txt (line 5))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting numpy>=1.21.0 (from -r requirements.txt (line 4))\n",
      "  Using cached numpy-2.2.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn>=1.0.0->-r requirements.txt (line 8))\n",
      "  Using cached scipy-1.16.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=1.0.0->-r requirements.txt (line 8))\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.0.0->-r requirements.txt (line 8))\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.4.0->-r requirements.txt (line 9))\n",
      "  Using cached contourpy-1.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.4.0->-r requirements.txt (line 9))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.4.0->-r requirements.txt (line 9))\n",
      "  Using cached fonttools-4.59.0-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (107 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.4.0->-r requirements.txt (line 9))\n",
      "  Using cached kiwisolver-1.4.8-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env36/lib/python3.13/site-packages (from matplotlib>=3.4.0->-r requirements.txt (line 9)) (25.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.4.0->-r requirements.txt (line 9))\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1.0->-r requirements.txt (line 14))\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Requirement already satisfied: absl-py>=0.4 in ./env36/lib/python3.13/site-packages (from tensorboard>=2.7.0->-r requirements.txt (line 17)) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./env36/lib/python3.13/site-packages (from tensorboard>=2.7.0->-r requirements.txt (line 17)) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./env36/lib/python3.13/site-packages (from tensorboard>=2.7.0->-r requirements.txt (line 17)) (3.8.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./env36/lib/python3.13/site-packages (from tensorboard>=2.7.0->-r requirements.txt (line 17)) (6.31.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./env36/lib/python3.13/site-packages (from tensorboard>=2.7.0->-r requirements.txt (line 17)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./env36/lib/python3.13/site-packages (from tensorboard>=2.7.0->-r requirements.txt (line 17)) (3.1.3)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb>=0.12.0->-r requirements.txt (line 18))\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.12.0->-r requirements.txt (line 18))\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in ./env36/lib/python3.13/site-packages (from wandb>=0.12.0->-r requirements.txt (line 18)) (4.3.8)\n",
      "Collecting pydantic<3 (from wandb>=0.12.0->-r requirements.txt (line 18))\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./env36/lib/python3.13/site-packages (from wandb>=0.12.0->-r requirements.txt (line 18)) (2.32.4)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb>=0.12.0->-r requirements.txt (line 18))\n",
      "  Using cached sentry_sdk-2.34.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb>=0.12.0->-r requirements.txt (line 18))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb>=0.12.0->-r requirements.txt (line 18))\n",
      "  Using cached pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb>=0.12.0->-r requirements.txt (line 18))\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./env36/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb>=0.12.0->-r requirements.txt (line 18)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env36/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb>=0.12.0->-r requirements.txt (line 18)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env36/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb>=0.12.0->-r requirements.txt (line 18)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env36/lib/python3.13/site-packages (from requests<3,>=2.0.0->wandb>=0.12.0->-r requirements.txt (line 18)) (2025.8.3)\n",
      "Collecting huggingface_hub (from timm>=0.4.0->-r requirements.txt (line 22))\n",
      "  Using cached huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors (from timm>=0.4.0->-r requirements.txt (line 22))\n",
      "  Using cached safetensors-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics>=0.5.0->-r requirements.txt (line 23))\n",
      "  Using cached lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting iniconfig>=1 (from pytest>=6.2.0->-r requirements.txt (line 26))\n",
      "  Using cached iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest>=6.2.0->-r requirements.txt (line 26))\n",
      "  Using cached pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pygments>=2.7.2 in ./env36/lib/python3.13/site-packages (from pytest>=6.2.0->-r requirements.txt (line 26)) (2.19.2)\n",
      "Collecting mypy-extensions>=0.4.3 (from black>=21.9.0->-r requirements.txt (line 27))\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black>=21.9.0->-r requirements.txt (line 27))\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting mccabe<0.8.0,>=0.7.0 (from flake8>=3.9.0->-r requirements.txt (line 28))\n",
      "  Using cached mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pycodestyle<2.15.0,>=2.14.0 (from flake8>=3.9.0->-r requirements.txt (line 28))\n",
      "  Using cached pycodestyle-2.14.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting pyflakes<3.5.0,>=3.4.0 (from flake8>=3.9.0->-r requirements.txt (line 28))\n",
      "  Using cached pyflakes-3.4.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting jupyter-console (from jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nbconvert (from jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: ipykernel in ./env36/lib/python3.13/site-packages (from jupyter>=1.0.0->-r requirements.txt (line 32)) (6.30.1)\n",
      "Collecting jupyterlab (from jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached jupyterlab-4.4.5-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim<0.3,>=0.2 (from notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in ./env36/lib/python3.13/site-packages (from notebook>=6.4.0->-r requirements.txt (line 33)) (6.5.1)\n",
      "Collecting anyio>=3.1.0 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in ./env36/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./env36/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33)) (5.8.1)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting nbformat>=5.3.0 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pyzmq>=24 in ./env36/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33)) (27.0.1)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in ./env36/lib/python3.13/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33)) (5.14.3)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting httpx>=0.25.0 (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached jupyter_lsp-2.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting jsonschema>=4.18.0 (from jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached jsonschema-4.25.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./env36/lib/python3.13/site-packages (from ipywidgets>=7.6.0->-r requirements.txt (line 34)) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./env36/lib/python3.13/site-packages (from ipywidgets>=7.6.0->-r requirements.txt (line 34)) (9.4.0)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets>=7.6.0->-r requirements.txt (line 34))\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets>=7.6.0->-r requirements.txt (line 34))\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.9.0->-r requirements.txt (line 38))\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in ./env36/lib/python3.13/site-packages (from onnxruntime>=1.9.0->-r requirements.txt (line 38)) (25.2.10)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi>=0.70.0->-r requirements.txt (line 41))\n",
      "  Using cached starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting sniffio>=1.1 (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.15.0->-r requirements.txt (line 42))\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./env36/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 43)) (1.6.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./env36/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 43)) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in ./env36/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 43)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./env36/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 43)) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in ./env36/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 43)) (3.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./env36/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 43)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./env36/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 43)) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./env36/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 43)) (1.17.2)\n",
      "Requirement already satisfied: keras>=3.10.0 in ./env36/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 43)) (3.11.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./env36/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 43)) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in ./env36/lib/python3.13/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 43)) (0.5.3)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./env36/lib/python3.13/site-packages (from astunparse>=1.6.0->tensorflow>=2.13.0->-r requirements.txt (line 43)) (0.45.1)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.12.0->-r requirements.txt (line 18))\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.12.0->-r requirements.txt (line 18))\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./env36/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 32)) (1.8.16)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./env36/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 32)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in ./env36/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 32)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in ./env36/lib/python3.13/site-packages (from ipykernel->jupyter>=1.0.0->-r requirements.txt (line 32)) (7.0.0)\n",
      "Requirement already satisfied: decorator in ./env36/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./env36/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./env36/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./env36/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./env36/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (3.0.51)\n",
      "Requirement already satisfied: stack_data in ./env36/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in ./env36/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./env36/lib/python3.13/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env36/lib/python3.13/site-packages (from jinja2->torch>=1.9.0->-r requirements.txt (line 2)) (3.0.2)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached rpds_py-0.26.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rfc3987-syntax>=1.1.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached rfc3987_syntax-1.1.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: rich in ./env36/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow>=2.13.0->-r requirements.txt (line 43)) (14.1.0)\n",
      "Requirement already satisfied: namex in ./env36/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow>=2.13.0->-r requirements.txt (line 43)) (0.1.0)\n",
      "Requirement already satisfied: optree in ./env36/lib/python3.13/site-packages (from keras>=3.10.0->tensorflow>=2.13.0->-r requirements.txt (line 43)) (0.17.0)\n",
      "Collecting beautifulsoup4 (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting defusedxml (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached mistune-3.1.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached nbclient-0.10.2-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting fastjsonschema>=2.15 (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./env36/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (0.7.0)\n",
      "Collecting lark>=1.2.2 (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.9.0->-r requirements.txt (line 2))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting cffi>=1.0.1 (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached cffi-1.17.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r requirements.txt (line 32))\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.9.0->-r requirements.txt (line 38))\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub->timm>=0.4.0->-r requirements.txt (line 22))\n",
      "  Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook>=6.4.0->-r requirements.txt (line 33))\n",
      "  Using cached types_python_dateutil-2.9.0.20250708-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./env36/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow>=2.13.0->-r requirements.txt (line 43)) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./env36/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow>=2.13.0->-r requirements.txt (line 43)) (0.1.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./env36/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./env36/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./env36/lib/python3.13/site-packages (from stack_data->ipython>=6.1.0->ipywidgets>=7.6.0->-r requirements.txt (line 34)) (0.2.3)\n",
      "Downloading torch-2.8.0-cp313-cp313-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.8/887.9 MB\u001b[0m \u001b[31m450.6 kB/s\u001b[0m eta \u001b[36m0:15:13\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Connection timed out while downloading.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mincomplete-download\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Download failed because not enough bytes were received (476.8 MB/887.9 MB)\n",
      "\u001b[31m╰─>\u001b[0m URL: \u001b[4;94mhttps://files.pythonhosted.org/packages/16/82/3948e54c01b2109238357c6f86242e6ecbf0c63a1af46906772902f82057/torch-2.8.0-cp313-cp313-manylinux_2_28_x86_64.whl\u001b[0m\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with network connectivity, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: Consider using --resume-retries to enable download resumption.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-05T08:30:41.425882Z",
     "iopub.status.busy": "2025-08-05T08:30:41.425549Z",
     "iopub.status.idle": "2025-08-05T08:30:41.432141Z",
     "shell.execute_reply": "2025-08-05T08:30:41.431380Z",
     "shell.execute_reply.started": "2025-08-05T08:30:41.425856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mL\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "\n",
    "# Tensor flow addition feature: algo, loss function, \n",
    "# import tensorflow_addons as tfa\n",
    "import glob, random, os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use classification_report for report the f1 score, loss, percisetense, etc \n",
    "# Create confuse matrix to regard the modle performance \n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print('TensorFlow Version ' + tf.__version__)\n",
    "\n",
    "\n",
    "# Seed the random to consist every train to predictable random value \n",
    "def seed_everything(seed = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed_everything()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T08:30:51.247041Z",
     "iopub.status.busy": "2025-08-05T08:30:51.246754Z",
     "iopub.status.idle": "2025-08-05T08:30:51.267397Z",
     "shell.execute_reply": "2025-08-05T08:30:51.266826Z",
     "shell.execute_reply.started": "2025-08-05T08:30:51.247017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "batch_size = 16\n",
    "n_classes = 5\n",
    "\n",
    "train_path = './data/processed/train/'\n",
    "test_path = './data/processed/test'\n",
    "val_path = './data/processed/val/'\n",
    "\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('./data/metadata/train_metadata.csv', dtype = 'str')\n",
    "\n",
    "df_val= pd.read_csv('./data/metadata/val_metadata.csv', dtype = 'str')\n",
    "df_test = pd.read_csv('./data/metadata/test_metadata.csv', dtype = 'str')\n",
    "\n",
    "\n",
    "classes = {0 : \"COVID\",\n",
    "           1 : \"Lung_Opacity\",\n",
    "           2 : \"Normal\",\n",
    "           3 : \"Viral_Pneumonia\",\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T08:34:49.027381Z",
     "iopub.status.busy": "2025-08-05T08:34:49.027013Z",
     "iopub.status.idle": "2025-08-05T08:34:49.033324Z",
     "shell.execute_reply": "2025-08-05T08:34:49.032494Z",
     "shell.execute_reply.started": "2025-08-05T08:34:49.027330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def data_augment(image):\n",
    "    p_spatial = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    "    p_rotate = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n",
    " \n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    if p_spatial > .75:\n",
    "        image = tf.image.transpose(image)\n",
    "        \n",
    "    # Rotates\n",
    "    if p_rotate > .75:\n",
    "        image = tf.image.rot90(image, k = 3) # rotate 270º\n",
    "    elif p_rotate > .5:\n",
    "        image = tf.image.rot90(image, k = 2) # rotate 180º\n",
    "    elif p_rotate > .25:\n",
    "        image = tf.image.rot90(image, k = 1) # rotate 90º\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T08:46:36.558918Z",
     "iopub.status.busy": "2025-08-05T08:46:36.558597Z",
     "iopub.status.idle": "2025-08-05T08:48:10.694954Z",
     "shell.execute_reply": "2025-08-05T08:48:10.694138Z",
     "shell.execute_reply.started": "2025-08-05T08:46:36.558888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Perform gernate image after normalize iamge and data argument \n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(samplewise_center = True,\n",
    "                                                          samplewise_std_normalization = True,\n",
    "                                                          # Extract data for validation part \n",
    "                                                          validation_split = 0.2,\n",
    "                                                          preprocessing_function = data_augment)\n",
    "# Extract the training image data as generator \n",
    "train_gen = datagen.flow_from_dataframe(dataframe = df_train,\n",
    "                                        # From data frame training we show the training file path to extract to image 2D value \n",
    "                                        directory = \".\",\n",
    "                                        # Image path feature name for image path \n",
    "                                        x_col = 'file_path',\n",
    "                                        # Name label in data frame for image label \n",
    "                                        y_col = 'class',\n",
    "                                        # Get the subset folder \n",
    "                                        # subset = 'training',\n",
    "                                        batch_size = batch_size,\n",
    "                                        seed = 1,\n",
    "                                        color_mode = 'rgb',\n",
    "                                        shuffle = True,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        target_size = (image_size, image_size))\n",
    "# Extract the validation part \n",
    "valid_gen = datagen.flow_from_dataframe(dataframe = df_val,\n",
    "                                        directory = \".\",\n",
    "                                        x_col = 'file_path',\n",
    "                                        y_col = 'class',\n",
    "                                        # Extract from validation sub folder \n",
    "                                        # subset = 'validation',\n",
    "                                        batch_size = batch_size,\n",
    "                                        seed = 1,\n",
    "                                        color_mode = 'rgb',\n",
    "                                        shuffle = False,\n",
    "                                        class_mode = 'categorical',\n",
    "                                        target_size = (image_size, image_size))\n",
    "\n",
    "test_gen = datagen.flow_from_dataframe(dataframe = df_test,\n",
    "                                       x_col = 'file_path',\n",
    "                                       y_col = None,\n",
    "                                       batch_size = batch_size,\n",
    "                                       seed = 1,\n",
    "                                       color_mode = 'rgb',\n",
    "                                       shuffle = False,\n",
    "                                       class_mode = None,\n",
    "                                       target_size = (image_size, image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Images Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T08:49:19.494968Z",
     "iopub.status.busy": "2025-08-05T08:49:19.494613Z",
     "iopub.status.idle": "2025-08-05T08:49:26.437282Z",
     "shell.execute_reply": "2025-08-05T08:49:26.436403Z",
     "shell.execute_reply.started": "2025-08-05T08:49:19.494938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get 16 image from train data to preview \n",
    "\n",
    "images = [train_gen[0][0][i] for i in range(16)]\n",
    "# Create plot figure 3x5 grid each size 10px square\n",
    "fig, axes = plt.subplots(3, 5, figsize = (10, 10))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "for img, ax in zip(images, axes):\n",
    "    # Make sure the right size \n",
    "    ax.imshow(img.reshape(image_size, image_size, 3))\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:09:39.496551Z",
     "iopub.status.busy": "2025-08-05T09:09:39.496265Z",
     "iopub.status.idle": "2025-08-05T09:09:39.710842Z",
     "shell.execute_reply": "2025-08-05T09:09:39.709898Z",
     "shell.execute_reply.started": "2025-08-05T09:09:39.496526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "a = np.array(train_gen[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:11:12.637865Z",
     "iopub.status.busy": "2025-08-05T09:11:12.637530Z",
     "iopub.status.idle": "2025-08-05T09:11:12.642170Z",
     "shell.execute_reply": "2025-08-05T09:11:12.641247Z",
     "shell.execute_reply.started": "2025-08-05T09:11:12.637832Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 1\n",
    "\n",
    "patch_size = 8  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [56, 28]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model and it's Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:11:16.315419Z",
     "iopub.status.busy": "2025-08-05T09:11:16.315112Z",
     "iopub.status.idle": "2025-08-05T09:11:16.319716Z",
     "shell.execute_reply": "2025-08-05T09:11:16.318859Z",
     "shell.execute_reply.started": "2025-08-05T09:11:16.315394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    # For each hidene unit (list) we create perspective number of dense and dropout \n",
    "    for units in hidden_units:\n",
    "        x = L.Dense(units, activation = tf.nn.gelu)(x)\n",
    "        x = L.Dropout(dropout_rate)(x)\n",
    "    # Return the value after go though the layers \n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Patch Creation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:11:40.141469Z",
     "iopub.status.busy": "2025-08-05T09:11:40.141160Z",
     "iopub.status.idle": "2025-08-05T09:11:40.146804Z",
     "shell.execute_reply": "2025-08-05T09:11:40.146000Z",
     "shell.execute_reply.started": "2025-08-05T09:11:40.141440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Patches(L.Layer):\n",
    "\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        # Number of image as batch size \n",
    "        batch_size = tf.shape(images)[0]\n",
    "        # Reshape to list of patchs \n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images,\n",
    "            sizes = [1, self.patch_size, self.patch_size, 1],\n",
    "            # No overlapp each patch \n",
    "            strides = [1, self.patch_size, self.patch_size, 1],\n",
    "            rates = [1, 1, 1, 1],\n",
    "            padding = 'VALID',\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Image Patches Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:13:42.838818Z",
     "iopub.status.busy": "2025-08-05T09:13:42.838483Z",
     "iopub.status.idle": "2025-08-05T09:14:08.010347Z",
     "shell.execute_reply": "2025-08-05T09:14:08.009434Z",
     "shell.execute_reply.started": "2025-08-05T09:13:42.838788Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "x = train_gen.next()\n",
    "image = x[0][0]\n",
    "\n",
    "plt.imshow(image.astype('uint8'))\n",
    "plt.axis('off')\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size = (image_size, image_size)\n",
    ")\n",
    "\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f'Image size: {image_size} X {image_size}')\n",
    "print(f'Patch size: {patch_size} X {patch_size}')\n",
    "print(f'Patches per image: {patches.shape[1]}')\n",
    "print(f'Elements per patch: {patches.shape[-1]}')\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype('uint8'))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Patch Encoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:32:05.405353Z",
     "iopub.status.busy": "2025-08-05T09:32:05.405013Z",
     "iopub.status.idle": "2025-08-05T09:32:05.410880Z",
     "shell.execute_reply": "2025-08-05T09:32:05.409857Z",
     "shell.execute_reply.started": "2025-08-05T09:32:05.405312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(L.Layer):\n",
    "    \n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        # Project will accept htil\n",
    "        self.projection = L.Dense(units = projection_dim)\n",
    "        # Position encoding each image patch \n",
    "        self.position_embedding = L.Embedding(\n",
    "            # Embeding reccept patch and project (down dimension) to vector projection_dimension \n",
    "            input_dim = num_patches, output_dim = projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        # Concatenation for position encoding \n",
    "        positions = tf.range(start = 0, limit = self.num_patches, delta = 1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:32:06.503441Z",
     "iopub.status.busy": "2025-08-05T09:32:06.503135Z",
     "iopub.status.idle": "2025-08-05T09:32:06.510030Z",
     "shell.execute_reply": "2025-08-05T09:32:06.509300Z",
     "shell.execute_reply.started": "2025-08-05T09:32:06.503413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def vision_transformer():\n",
    "    # Define the input shape \n",
    "    inputs = L.Input(shape = (image_size, image_size, 3))\n",
    "    \n",
    "    # Create patches. => tranfer input to the patch layer \n",
    "    # Contruct define the patch size \n",
    "    # Then call by passing the input from previous layer \n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    \n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "    \n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        \n",
    "        # Layer normalization 1.\n",
    "        x1 = L.LayerNormalization(epsilon = 1e-6)(encoded_patches)\n",
    "        \n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = L.MultiHeadAttention(\n",
    "            num_heads = num_heads, key_dim = projection_dim, dropout = 0.1\n",
    "        )(x1, x1)\n",
    "        \n",
    "        # Skip connection 1.\n",
    "        x2 = L.Add()([attention_output, encoded_patches])\n",
    "        \n",
    "        # Layer normalization 2.\n",
    "        x3 = L.LayerNormalization(epsilon = 1e-6)(x2)\n",
    "        \n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units = transformer_units, dropout_rate = 0.1)\n",
    "        \n",
    "        # Skip connection 2.\n",
    "        encoded_patches = L.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = L.LayerNormalization(epsilon = 1e-6)(encoded_patches)\n",
    "    representation = L.Flatten()(representation)\n",
    "    representation = L.Dropout(0.5)(representation)\n",
    "    \n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units = mlp_head_units, dropout_rate = 0.5)\n",
    "    \n",
    "    # Classify outputs.\n",
    "    logits = L.Dense(n_classes)(features)\n",
    "    \n",
    "    # Create the model.\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = logits)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:32:08.229040Z",
     "iopub.status.busy": "2025-08-05T09:32:08.228739Z",
     "iopub.status.idle": "2025-08-05T09:32:08.232672Z",
     "shell.execute_reply": "2025-08-05T09:32:08.231917Z",
     "shell.execute_reply.started": "2025-08-05T09:32:08.229015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Total batch perform calc from number of image and batch size \n",
    "decay_steps = train_gen.n // train_gen.batch_size\n",
    "initial_learning_rate = learning_rate\n",
    "\n",
    "\n",
    "# Learning rate is the consine funciton base on decay_steps \n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecay(initial_learning_rate, decay_steps)\n",
    "# Make schedule training from this consine decays function for trainning \n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_decayed_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-05T09:32:11.317445Z",
     "iopub.status.busy": "2025-08-05T09:32:11.317157Z",
     "iopub.status.idle": "2025-08-05T09:42:09.460679Z",
     "shell.execute_reply": "2025-08-05T09:42:09.459348Z",
     "shell.execute_reply.started": "2025-08-05T09:32:11.317420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "model = vision_transformer()\n",
    "    \n",
    "model.compile(optimizer = optimizer, \n",
    "              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.1), \n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size\n",
    "STEP_SIZE_VALID = valid_gen.n // valid_gen.batch_size\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
    "                                                 min_delta = 1e-4,\n",
    "                                                 patience = 5,\n",
    "                                                 mode = 'max',\n",
    "                                                 restore_best_weights = True,\n",
    "                                                 verbose = 1)\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = './model.hdf5',\n",
    "                                                  monitor = 'val_accuracy', \n",
    "                                                  verbose = 1, \n",
    "                                                  save_best_only = True,\n",
    "                                                  save_weights_only = True,\n",
    "                                                  mode = 'max')\n",
    "\n",
    "callbacks = [earlystopping, lr_scheduler, checkpointer]\n",
    "\n",
    "model.fit(x = train_gen,\n",
    "          steps_per_epoch = STEP_SIZE_TRAIN,\n",
    "          validation_data = valid_gen,\n",
    "          validation_steps = STEP_SIZE_VALID,\n",
    "          epochs = num_epochs,\n",
    "          callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print('Training results')\n",
    "model.evaluate(train_gen)\n",
    "\n",
    "print('Validation results')\n",
    "model.evaluate(valid_gen)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1718836,
     "sourceId": 13836,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30061,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
